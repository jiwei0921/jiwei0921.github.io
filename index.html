<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="UTF-8">
  <title>Wei Ji</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wei Ji</name>
              </p>
              <p>I am now a Ph.D. student in Computer Engineering at University of Alberta, where I am fortunate to be supervised by <a href="https://scholar.google.com/citations?user=9IRFiEQAAAAJ&hl=en">Prof. Li Cheng</a> in VLLab. I also worked as a visiting Ph.D. student at Johns Hopkins University, collaborating with <a href="https://www.zongweiz.com/">Dr. Zongwei Zhou</a> and <a href="https://scholar.google.com/citations?user=FJ-huxgAAAAJ&hl=en">Prof. Alan Yuille</a>. In addition, I have gained industry experience at Samsung Research America, ByteDance, and Tencent.
              </p>
              <p>
                My research focuses on the fundamental aspects of Computer Vision, particularly in the areas of detection, segmentation, and multi-modal robust learning. My goal is to develop intelligent visual perception systems capable of accurately modeling scene representation and widely applying to real-world scenarios. Application cases of my work include medical diagnosis systems, robot obstacle avoidance, safer autonomous vehicles, and portrait editor tools.
              </p>
              <p style="text-align:center">
                <a href="mailto:wji3@ualberta.ca">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=G4uCKHcAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/jiwei0921/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/wei.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/wei.png" class="hoverZoomLink"></a>
            </td>
          </tr>
       

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Recent News</heading>
            <ul>
                <li>One paper is accepted by <b>CVPR 2023</b>.</li>
                <li>Awarded the Alberta Innovates Graduate Student Scholarship.</li>
                <li>I passed my Ph.D. Candidacy Oral exam.</li>
                <li>One paper is accepted by <b>IJCV</b>.</li>
                <li>Achieved the Floyd Derkat Graduate Award in Artificial Intelligence and Machine Learning.</li>
                <li>Two papers are accepted by <b>ICLR 2022</b> and <b>CVPR 2022</b>.</li>
              </div>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
	            <p>
                <a href="https://scholar.google.com/citations?user=G4uCKHcAAAAJ">Full list of publised papers &#8594;</a>
                </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    


          <tr onmouseout="MVSS_stop()" onmouseover="MVSS_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='MVSS_image'>
                  <img src='images/MVSS_after.png' width="160"></div>
                <img src='images/MVSS_before.png' width="160">
              </div>
              <script type="text/javascript">
                function MVSS_start() {
                  document.getElementById('MVSS_image').style.opacity = "1";
                }

                function MVSS_stop() {
                  document.getElementById('MVSS_image').style.opacity = "0";
                }
                MVSS_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.07179.pdf">
                <papertitle>Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline</papertitle>
              </a>
              <br>
              <strong>Wei Ji</strong>,
              Jingjing Li, Cheng Bian, Zongwei Zhou, Jiaying Zhao, Alan Yuille, Li Cheng
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://jiwei0921.github.io/Multispectral-Video-Semantic-Segmentation/">project website</a>
              /
              <a href="https://arxiv.org/pdf/2205.07179.pdf">paper</a>
               /
               <a href="https://github.com/jiwei0921/Multispectral-Video-Semantic-Segmentation">code repository</a>
              <p></p>
              <p>
                  We set out to address a relatively new task of semantic segmentation of multispectral video input, i.e., MVSS. To this end, an in-house MVSeg benchmark dataset and a dedicated baseline are curated and developed. This is expected to serve as a good starting point for future advancements in the MVSS task.
              </p>
            </td>
          </tr>

            <tr onmouseout="rgbt_stop()" onmouseover="rgbt_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='rgbt_image'>
                      <img src='images/rgbt_after.png' width="160"></div>
                    <img src='images/rgbt_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function rgbt_start() {
                      document.getElementById('rgbt_image').style.opacity = "1";
                    }
    
                    function rgbt_stop() {
                      document.getElementById('rgbt_image').style.opacity = "0";
                    }
                    rgbt_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2205.07179.pdf">
                    <papertitle>SemanticRT: Explicit Complement Modeling for RGB-Thermal Semantic Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Wei Ji</strong>,
                  Jingjing Li, Cheng Bian, Zhicheng Zhang, Li Cheng
                  <br>
                  <em>IJCV</em>, 2023 (Major revision)
                  <br>
                  <a href="https://jiwei0921.github.io/SemanticRT">project website</a>
                   /
                   <a href="https://arxiv.org/pdf/2205.07179.pdf">paper</a>
                   /
                   <a href="https://github.com/jiwei0921/SemanticRT/">code repository</a>
                  <p></p>
                  <p>
                      We propose an explicit mechanism toward better modeling of the innate multi-modality characteristics from the RGB and thermal inputs. Meanwhile, we release a large-scale RGB-Thermal semantic segmentation dataset, SemanticRT, with 11,371 pairs, toward promoting entire and robust scene understanding, especially under adverse conditions.                  </p>
                </td>
              </tr>



          
            <tr onmouseout="DSU_stop()" onmouseover="DSU_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='DSU_image'>
                      <img src='images/DSU_after.png' width="160"></div>
                    <img src='images/DSU_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function DSU_start() {
                      document.getElementById('DSU_image').style.opacity = "1";
                    }
    
                    function DSU_stop() {
                      document.getElementById('DSU_image').style.opacity = "0";
                    }
                    DSU_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2205.07179.pdf">
                    <papertitle>Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection</papertitle>
                  </a>
                  <br>
                  <strong>Wei Ji</strong>,
                  Jingjing Li, Qi Bi, Chuan Guo, Jie Liu, Li Cheng
                  <br>
                  <em>ICLR</em>, 2022
                  <br>
                   <a href="https://arxiv.org/pdf/2205.07179.pdf">paper</a>
                   /
                   <a href="https://github.com/jiwei0921/DSU/">code repository</a>
                  <p></p>
                  <p>
                      We tackle the new task of deep unsupervised RGB-D saliency detection, where depth information is internally engaged and refined to capture trustworthy supervision signals without introducing human efforts.
                  </p>
                </td>
              </tr>



              <tr onmouseout="MR_stop()" onmouseover="MR_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='MR_image'>
                      <img src='images/MR_after.png' width="160"></div>
                    <img src='images/MR_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function MR_start() {
                      document.getElementById('MR_image').style.opacity = "1";
                    }
    
                    function MR_stop() {
                      document.getElementById('MR_image').style.opacity = "0";
                    }
                    MR_stop()
                  </script>
                </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf">
                <papertitle>Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling</papertitle>
              </a>
              <br>
              <strong>Wei Ji</strong>,
              Shuang Yu, Junde Wu, Kai Ma, Cheng Bian, Qi Bi, Jingjing Li,
              Hanruo Liu, Li Cheng, Yefeng Zheng
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Best Paper Candidate)</strong></font>
              <br>
               <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf">paper</a>
               /
               <a href="https://github.com/jiwei0921/MRNet/">code repository</a>
               /
               <a href="https://www.bilibili.com/video/BV1WQ4y1Y7ed/">invited talk in China</a>
              <p></p>
              <p>
                We propose to explicitly model the multi-rater agreement regarding medical images, where it integrates varied expertise-level of individual raters, and can produce corresponding calibrated predictions that better reflect the underlying graders‚Äô (dis-)agreement.
              </p>
            </td>
          </tr>		

          <tr onmouseout="DCF_stop()" onmouseover="DCF_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DCF_image'>
                  <img src='images/DCF_after.png' width="160"></div>
                <img src='images/DCF_before.png' width="160">
              </div>
              <script type="text/javascript">
                function DCF_start() {
                  document.getElementById('DCF_image').style.opacity = "1";
                }

                function DCF_stop() {
                  document.getElementById('DCF_image').style.opacity = "0";
                }
                DCF_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Calibrated_RGB-D_Salient_Object_Detection_CVPR_2021_paper.pdf">
                <papertitle>Calibrated RGB-D Salient Object Detection</papertitle>
              </a>
              <br>
              <strong>Wei Ji</strong>,
              Jingjing Li, Shuang Yu, Miao Zhang, Yongri Piao, Shunyu Yao, Qi Bi, Yefeng Zheng, Huchuan Lu, Li Cheng
              <br>
              <em>CVPR</em>, 2021 &nbsp (Extended to IJCV)
              <br>
               <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Calibrated_RGB-D_Salient_Object_Detection_CVPR_2021_paper.pdf">paper</a>
               /
               <a href="https://link.springer.com/article/10.1007/s11263-022-01734-1">long version</a>
               /
               <a href="https://github.com/jiwei0921/DCF">code repository</a>
              <p></p>
              <p>
                We propose a Depth Calibration and Fusion (DCF) method, which is able to correct the latent bias in the original depth maps, and generate an optimal calibration of the depth values that directly promotes detection accuracy. Its effectiveness and scalability are demonstrated with extensive experiments on five public benchmarks. </p>
            </td>
          </tr>


            <tr onmouseout="CAT_stop()" onmouseover="CAT_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cat_image'>
                  <img src='images/cat_after.png' width="160"></div>
                <img src='images/cat_before.png' width="160">
              </div>
              <script type="text/javascript">
                function CAT_start() {
                  document.getElementById('cat_image').style.opacity = "1";
                }

                function CAT_stop() {
                  document.getElementById('cat_image').style.opacity = "0";
                }
                CAT_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2021/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf">
                <papertitle>Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection</papertitle>
              </a>
              <br>
              Jingjing Li, <strong>Wei Ji*</strong>,
              Qi Bi, Cheng Yan, Miao Zhang, Yongri Piao, Huchuan Lu, Li Cheng
              <br>
              <em>NeurIPS</em>, 2021
              <br>
               <a href="https://proceedings.neurips.cc/paper/2021/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf">paper</a>
               /
               <a href="https://github.com/jiwei0921/JSM">code repository</a>
              <p></p>
              <p>
                We attempt to tackle the new problem of weakly-supervised RGB-D salient object detection, where low-cost spatial and textual semantics are used to provide trustworthy training signals. Our framework is flexible and can be easily adapted to other fully-/un-supervised settings.
              </p>
            </td>
          </tr>


          <tr onmouseout="MICCAI_stop()" onmouseover="MICCAI_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='micca_image'>
                  <img src='images/miccai_after.png' width="160"></div>
                <img src='images/miccai_before.png' width="160">
              </div>
              <script type="text/javascript">
                function MICCAI_start() {
                  document.getElementById('micca_image').style.opacity = "1";
                }

                function MICCAI_stop() {
                  document.getElementById('micca_image').style.opacity = "0";
                }
                MICCAI_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-87237-3_6">
                <papertitle>Local-Global Dual Perception Based Deep Multiple Instance Learning for Retinal Disease Classification</papertitle>
              </a>
              <br>
              Qi Bi, Shuang Yu, <strong>Wei Ji</strong>,
              Cheng Bian, Lijun Gong, Hanruo Liu, Kai Ma, Yefeng Zheng
              <br>
              <em>MICCAI</em>, 2021 &nbsp <font color="red"><strong>(Young Scientist Award Nomination)</strong></font>
              <br>
               <a href="https://link.springer.com/chapter/10.1007/978-3-030-87237-3_6">paper</a>
               /
               <a href="https://miccai2021.org/openaccess/paperlinks/2021/09/01/294-Paper0778.html">open reviews</a>
              <p></p>
              <p>
                We propose an effective deep multiple instance learning framework that integrates the instance contribution from both local and global scales. It has been demonstrated to be scalable and versatile across different CNNs and medical tasks.
              </p>
            </td>
          </tr>


          <tr onmouseout="ECCV_stop()" onmouseover="ECCV_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ECCV_image'>
                  <img src='images/conet_after.png' width="160"></div>
                <img src='images/conet_before.png' width="160">
              </div>
              <script type="text/javascript">
                function ECCV_start() {
                  document.getElementById('ECCV_image').style.opacity = "1";
                }

                function ECCV_stop() {
                  document.getElementById('ECCV_image').style.opacity = "0";
                }
                ECCV_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2007.11782.pdf">
                <papertitle>Accurate RGB-D Salient Object Detection via Collaborative Learning</papertitle>
              </a>
              <br>
              <strong>Wei Ji</strong>,
              Jingjing Li, Miao Zhang, Yongri Piao, Huchuan Lu
              <br>
              <em>ECCV</em>, 2020
              <br>
               <a href="https://arxiv.org/pdf/2007.11782.pdf">paper</a>
               /
               <a href="https://github.com/jiwei0921/CoNet/">code repository</a>
              <p></p>
              <p>
                We introduce a collaborative learning scheme where three mutual-benefit tasks (saliency, depth, and edge) work together to improve internal characteristics. This results in accurate detection with sharp boundaries and faster speeds, while avoiding extra depth input during inference.
              </p>
            </td>
          </tr>


          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mira_image'>
                  <img src='images/mira_after.png' width="160"></div>
                <img src='images/mira_before.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9729103">
                <papertitle>Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection</papertitle>
              </a>
              <br>
              <strong>Wei Ji</strong>,
              Ge Yan, Jingjing Li, Yongri Piao, Miao Zhang, Li Cheng, Huchuan Lu
              <br>
              <em>ICCV</em>, 2019 &nbsp (Extended to IEEE TIP)
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/9729103">long version</a>
               /
               <a href="https://github.com/jiwei0921/DMRA">code repository</a>
              <p></p>
              <p>
                We propose a novel attention mechanism that can progressively optimize local details with memory-oriented scene understanding for generating appealing results. Its effectiveness is demonstrated on nine public benchmark datasets.
              </p>
            </td>
          </tr>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
              <td width="100%" valign="middle">
                  <heading>Selected Awards</heading>
                  <!-- <br /> -->
              <ul>
                <li>Alberta Innovates Graduate Student Scholarship ($62,000CAD), 2023</li>
              </ul>
              <ul>
                <li>Floyd Derkat Graduate Award in Artificial Intelligence and Machine Learning ($5,000CAD), 2022</li>
              </ul>
              <ul>
                <li>CVPR Best Paper Candidate, 2021</li>
              </ul>
              <ul>
                <li>MICCAI Young Scientist Award Nomination, 2021</li>
              </ul>
              <ul>
                <li>Rank 1st on MICCAI Challenge Leaderboard of QUBIQ Uncertainties Quantification, 2020</li>
              </ul>
              <ul>
                <li>Distinguished Graduate Thesis of Dalian University of Technology, 2019</li>
              </ul>
              <ul>
                <li>National First Prize of `China Graduate Contest on Application, Design and Innovation of Mobile-Terminal`, 2017</li>
              </ul>    
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                  This website template is borrowed from <a href="https://jonbarron.info/">Jon Barron's Website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
